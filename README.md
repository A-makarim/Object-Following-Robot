# ğŸ¤– Autonomous Human & Object Following Robot

An **autonomous robot** capable of **real-time object and human tracking**, built using the **Arduino Nicla Vision** platform.  
The system leverages on-board computer vision, sensor fusion, and wireless communication to follow targets intelligently while monitoring its position through odometry.

---

![Arduino](https://img.shields.io/badge/Arduino-Nicla_Vision-00979D?logo=arduino&logoColor=white)
![Python](https://img.shields.io/badge/Python-3.x-3776AB?logo=python&logoColor=white)
![TinyML](https://img.shields.io/badge/TinyML-FOMO-orange)
![YOLOv11](https://img.shields.io/badge/YOLO-v11-yellow)
![WiFi](https://img.shields.io/badge/WiFi-Enabled-blue)
![License](https://img.shields.io/badge/License-MIT-green)

---

## ğŸš€ Features

- **ğŸ§  Real-Time Vision Processing**  
  Implemented object and human tracking on the **Arduino Nicla Vision** using its onboard **RGB camera** and **ToF (Time-of-Flight) sensor**.

- **ğŸ¯ Tracking Algorithms**  
  Designed a control system that determines robot movement based on **bounding box coordinates** and **ToF distance measurements**.

- **ğŸ¤– Machine Learning Integration**  
  Transitioned from traditional **color-based blob detection** to **TinyML-based recognition** using a **custom-trained FOMO model** for **human face detection**.

- **ğŸŒ Server-Side YOLOv11 Processing**  
  Experimented with **YOLOv11** for server-side face detection and comparison with embedded ML results.

- **ğŸ¦¾ Motion & Control**  
  Developed a **differential drive system** incorporating **odometry** for accurate position tracking and **real-time data visualization**.

- **ğŸ“¡ Wireless Communication**  
  Established **WiFi communication** between the Nicla Vision and the Arduino for coordinated control and data exchange.

---


---

## âš™ï¸ How It Works

1. **Detection** â€” The Nicla Vision captures frames via its RGB camera and processes them using a **FOMO TinyML model** for face/object recognition.  
2. **Distance Measurement** â€” The ToF sensor measures depth to determine proximity to the target.  
3. **Decision Making** â€” The bounding box position and ToF data are sent to the Arduino via WiFi.  
4. **Motion Control** â€” The Arduino computes control signals for the **differential drive motors** to follow the target.  
5. **Feedback** â€” Odometry and sensor data are visualized in real-time for debugging and analysis.

---

## ğŸ§° Hardware Used

- ğŸŸ¦ **Arduino Nicla Vision** (RGB Camera + ToF Sensor)  
- âš™ï¸ **Arduino Board** (for motion control & sensor integration)  
- ğŸ› **Differential Drive Motors** with encoders  
- ğŸ”‹ **Battery / Power Module**  
- ğŸ“¡ **WiFi Communication Module (Nicla Vision WiFi - Adafruit WINC1500 WiFi shielf)**
- ğŸ–µ **TFT display and LEDs to display robot status and different functions**

---

## ğŸ§  Software Stack

- **Arduino IDE / MicroPython (OpenMV IDE)**
- **YOLOv11 (Server-Side Testing)**
- **FOMO (TinyML Model)**
- **Edge Impulse / OpenCV (for training & evaluation)**
- **WiFi communication protocols (TCP/UDP)**
- **Python for integration & visualization**

---

## ğŸ§ª Results

âœ… Real-time face and object tracking  
âœ… Smooth following behavior based on ToF distance  
âœ… Reliable WiFi communication between Nicla Vision and Arduino  
âœ… Successful migration from basic blob detection â†’ advanced TinyML-based vision  

---



